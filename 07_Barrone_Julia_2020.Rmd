---
title: 'Homework #7'
author: "Julia Barrone"
date: "10/25/2020"
output: html_document
---

1. Let’s first look at the data. Plot it, along with a polynomial fit (remember, formula = y ~ poly(x,2) for a quadratic). Then, compare the r2 value of a linear versus fith order fit. What do you see?
```{r 1}
#load libraries that I need 
library(ggplot2)
library(dplyr)
library(ggpmisc)
library(tidyr)


#reading in the data 
prog <- read.csv("chap17q07ProgesteroneExercise.csv") 

#plotting the data with a polynomial fit 
ggplot(data = prog, 
       mapping = aes(x=progesterone,
                     y=ventilation))+
  #adding an r^2 equation to the plot for fifth order fit
  stat_poly_eq(formula = y~poly(x,5),
                parse = TRUE) +
  geom_point() +
  stat_smooth(method = "lm", formula = y~poly(x,5), col = "red")
#r-squared value is 0.27


#plotting the data with a linear fit 
ggplot(data = prog, 
       mapping = aes(x=progesterone,
                     y=ventilation))+
   #adding an r^2 equation to the plot
    stat_poly_eq(formula = y~x,
                parse = TRUE) +
  geom_point() +
  stat_smooth(method = "lm", col = "red")
#r-squared value is 0.24


#We see that the polynomial fifth order fit is a better fit to the data and that is shown by a slightly higher r^2 value of 0.27 vs 0.24 for the linear fit. 

```


2. Does that result hold up, or is it due to overfitting? Let’s evaluate by comparing 5-fold CV scores using RMSE. Let’s do this efficiently, though!

2A. Get things ready! Make a 5-fold cross validation tibble using rsample::vfold_cv() and then combine each possible fold with the polynomials 1:5 using tidyr::crossing()
```{r 2A}

#5 fold cross validation tibble using prog dataset 
prog_five_fold <- rsample::vfold_cv(prog, v=5) %>%
  
#combining each possible fold with the polynomials 1:5
    tidyr::crossing(p=1:5) 

prog_five_fold 

```


2B. Now you have splits and a column of coefficients. Use purr::map2() to make a list column of fit models, where you use the splits and data and the polynomials for you poly() call in the model.
```{r 2B}
library(rsample)
library(boot)
library(modelr)
library(purrr)


?map2

#create a new column, fit model, which we make with map2, iterating over all splits AND polynomials 
fit_mod_prog <- prog_five_fold  %>%
    mutate(fit_model = map2(splits,
                          p,
                         ~lm(ventilation ~ poly(progesterone, .y),
                         data = analysis(.x))))
#analysis gives us our testing data set   

fit_mod_prog
```


2C. Great! Now, calculate the rmse for each fold/polynomial combination as we did in lab.
```{r 2C}
  
#create a new column, rmse, which we make with map2, iterating over all splits and fit models 
rmse_prog <- fit_mod_prog  %>%
  mutate(rmse = map2_dbl(.x = splits, .y = fit_model,
                         ~rmse(model = .y,
                               data = assessment(.x))))
#assessment gives us our training data set 

rmse_prog
```


2D. Implications - ok, given that the 5-fold score is the average RMSE across all folds for a given polynomial, show in both a table and figure the relationship between polynomial and out-of-sample RMSE. What does this tell you?
```{r 2D}
#asking for a table showing the polynomials and five fold score column which was generated by getting the average rmse for each fold 
prog_avg_rmse <- rmse_prog %>%
  group_by(p) %>%
  summarise(five_fold_score = unique(mean(rmse)))

prog_avg_rmse

#plotting the relationship between the polunomials and rmse. I added the line to help follow the points more easily
ggplot(data = prog_avg_rmse,
       mapping = aes(x = p,
                     y = five_fold_score)) +
  geom_point() +
  geom_line()

#So rmse is the average magnitude of error. From this graph and table we can see that the first fold shows the lowest amount of error. 

```


3. That was all well and good, but, how to these results compare to doing this analysis with AIC using the {AICcmodavg} package? Note, you can use dplyr and purrr to not have to fit each model manually.
```{r 3}


#AIC ####
#linear fit 
prog_lm <- lm(ventilation ~ progesterone, data = prog)

#intercept only model 
prog_int <- lm(ventilation ~ 1, data = prog)

#polynomial fit second order
prog_sq <- lm(ventilation~ poly(progesterone, 2), data = prog)

#polynomial fit third order
prog_cube <- lm(ventilation ~ poly(progesterone, 3), data = prog)

#polynomial fit fourth order 
prog_quad <- lm(ventilation ~ poly(progesterone, 4), data = prog)  

#polynomial fit fifth order
prog_quin <- lm(ventilation ~ poly(progesterone, 5), data = prog)  

library(AICcmodavg)
#listing all the models together
mod_list <- list(prog_int, prog_lm, prog_sq, prog_cube, prog_quad, prog_quin)

#naming all the model in the list
name_vec <- c("int", "linear", "quad", "cube", "quad", "quin")

#finding our AIC scores for each model 
aictab(cand.set = mod_list, modnames = name_vec)

#From the AIC scores we can see the the linear model fits the data the best. That said, the AIC scores of the linear model and fourth fit polynomial weren't too different in terms of AIC score and thus fit. However, the fifth fit polynomial had a difference of 10.66 in AIC score from the linear model which indicates a worse fit. 


```


5. Last week, we did grid sampling with Likelihood. This week, let’s do it with Bayes!

p(H|D)=p(D|H)p(H)p(D)

5A.Let’s start with the Palmer Penguins data. Let’s look at just the Gentoo. Why don’t you plot the distribution of the average flipper length of females. We’ll use this data for the exercise. Remember to remove NAs - it will make the rest of the exercise easier. 1 EC for each thing you do to snaz the plot up.
```{r 5A}
#loading penguins data 
library(palmerpenguins)

#taking out the NAs of the penguin dataset
penguins_no_na <- na.omit(penguins)

#Filtering just Gentoo penguins in the dataset 
peng_gentoo <- penguins_no_na %>%
  filter(species == "Gentoo")

#filtering just females in the Gentoo penguins dataseet 
peng_gentoo_females <- peng_gentoo %>%
  filter(sex == "female")


#using ggplot to plot a histogram showing female flipper length 
ggplot(peng_gentoo_females,
       mapping=aes(x=flipper_length_mm)) + geom_histogram(aes(y=..density..), 
#Histogram with density instead of count on y-axis
                   binwidth=.5,
                   colour="black", fill="red") +
    geom_density(alpha=.6, fill="#FFFFFF") +
# Overlay with transparent density plot 

   geom_vline(aes(xintercept=mean(flipper_length_mm)), color="black", linetype="dashed", size=2) 
#Adding a line to show the mean of flipper length

mean(peng_gentoo_females$flipper_length_mm) 
#the mean flipper length for females is 212.7069

sd(peng_gentoo_females$flipper_length_mm) 
#standard deviation of flipper length for females is 3.897856
```


5B. OK, this is pretty normal, with a mean of 212.71 and sd of 3.9. Make a grid to search a number of values around that mean and SD, just as you did for likelihood. Let’s say 100 values of each parameter.
```{r 5B}

#making a grid using crossing and estimating my mean with values of 210-215, and estimating my sd with values of 3-4 each with a length of 100 to yield 10,000 rows
peng_grid_samp <- crossing(mean = seq(210,215, length.out = 100), 
                      sd = seq(3,4, length.out = 100)) 

peng_grid_samp 

```


5C. Write a function that will give you the numerator for any combination of m and s! This is just the same as writing a function for likelihood, but including an additional multiplier of p(H), when putting in the likelihood. Let’s assume a prior for m of dnorm(210, 50) and for s of dunif(1,10) - so, pretty weak!

So, we want p(m, s|flipper length)*p(m)*p(s).

BUT - small problem. These numbers get so vanishingly small, we can no longer do calculations with them. So, for any probability density you use, add log=TRUE and take a sum instead of products or multiplication, as

log(p(D|H)p(H))=log(p(D|H))+log(p(H))
```{r 5C}

#function to find numerator every time 
num_funct <- function(m, s){

sum(dnorm(peng_gentoo_females$flipper_length_mm, m, s, log = TRUE)) +
    #adding our priors
    dnorm(m, 210, 50, log = TRUE) + 
    #adding standard deviation
    dunif(s, 1,10, log = TRUE)


}

?dunif

```


5D. Great! Now use this function with your sample grid to get the numerator of the posterior, and then standardize with the p(D) - the sum of all numerators - to get a full posterior. Note, as we’re working in logs, we just subtract log(p(D)) What is the modal estimate of each parameter? How do they compare to the standard frequentist estimate?

Note: log(p(d)) = log(sum(exp(p(D|H)p(H))))
```{r 5D}
#going rowwise by mean and sd
peng_grid_samp <- peng_grid_samp %>%
  rowwise(mean,sd) %>%
  
  #creating a new column to show numerator using our function
  mutate(numerator = num_funct(m=mean, s=sd))


peng_log_post <- log(sum(exp(peng_grid_samp$numerator)))


#caluclating posterior for each row 
peng_grid_samp <- peng_grid_samp %>%
  rowwise() %>%
  #adding a log posterior and posterior column
  #
  mutate(log_posterior = numerator - peng_log_post, 
         
         #caluclating posterior by taking the exponential of the log posterior
         posterior = exp(log_posterior))

  
  
```

6 FINAL PROJECT
We’re at the half-way point in the course, and after the mid-term, it’s time to start thinking about your final project. So…. I want to know a bit about what you’re thinking of!

6A. What is the dataset you are thinking of working with? Tell me a bit about what’s in it, and where it comes from.
```{r 6A}

#For my thesis, I'm planning on analyzing Sphingidae occurrence datasets from North America. I'm not sure which dataset that I want to look at specifically for this project but I've compiled a list of 8 datasets so far from iNaturalist, Vermont Center for Ecostudies, MPM Milwaukee Public Museum Invertebrate Zoology Collection, The Lepidopterists' Society, Yale Peabody Museum, Museum of Comparative Zoology - Harvard University and Butterflies and Moths of North America and BugGuide. The datasets contain a mixture of historical specimen records, occurrence data from professionals and sightings recorded by citizen scientists. I've attached a summary table of what each data set contains and from briefly exploring the datasets, I've noticed that a lot of data cleaning is needed. Most of the datasets are missing IDs and record dates for some occurences so I will have to exclude them during my data cleaning process. I will also have to format dates so they will be in usable form(month and year in separate columns) Additionally, I will exclude species that don't have an adequate amount of records from my data analysis. I still have yet to decide what I want my base record amount to be for a species to be included in my data analysis. 

#Attached is the summary of all the datasets 
summary_of_datasets <- read.csv("Datasets Summary.csv")

summary_of_datasets

```


6B. What question do you want to ask of that data set?
```{r 6B}
#I want to see if any Sphingidae species have been advancing emergence times in the spring in recent years. There is a good amount of published literature about advancing phenology of Lepidoptera. My first step will be looking to see if I can find a trend in first appearance times over a span of years. I might find a negative trend(earlier emergence times), no trend(no change in emergence times) or postive trend(later emergene times). Then, I plan on comparing emergence time with many different variables to see if there is a correlation. Some example variables that I plan on using are temperature, precipitation, long/lat, etc. I'm curious to see what Sphingidae species are being affected and to see if I can quantify to what extent. I know the life histories of each species plays a role. I also know that my data will dictate what species I can include in my study since I need a certain amount of records for a species to perform data analysis accurately. To my knowledge, no study has specifically looked at Sphingidae phenology in North America. Most of the literature that I've reviewed are studies looking at Lepidoptera penology in Europe, which might yield different results than North America.     


```


EC C. Wanna make a quick visualization of some aspect of the data that might be provocative and interesting?
```{r 6C}
library(ggplot2)

#importing test data of Hemaris diffinis occurrences in the United States from BugGuide spanning from 1981-2019
hemaris_diffinis <-read.csv("Hemaris_BugGuide.csv")

str(hemaris_diffinis)
min(hemaris_diffinis$year)
max(hemaris_diffinis$year)


ggplot(hemaris_diffinis, 
       aes(x = year, y = month)) +
  geom_point(color='blue') +
  geom_smooth(method = "lm", se = FALSE)+
  facet_wrap(~stateProvince)


#The plot that I show here depicts year on x axis and month on y axis and each plot represents a different state. This plot provides good visualization to see in which states data is lacking. Some states have ample data points and other have only one or two data points. I've included a line of best fit in each plot to see if the sightings have advanced earlier as the years have gone by. The plots are clearly displaying a mix of positive, negative and neutral slopes. The next step would be to compare a variable, like temperature, for each state over the years, to see if that slope matches the slopes of appearance dates. The variation in the slopes might be caused by sampling effort differences in each state, observer bias, the region that each state is located in, or many other factors. I will have to account for bias in my data analysis. 

```

My github homework link is here: https://github.com/Jbarrone95/Biol-607-Homework- 